{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d71a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "from datetime import datetime, timedelta\n",
    "import pytz\n",
    "from zoneinfo import ZoneInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5389039f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name files \n",
    "#filename = '20220404_20220408_Modesto.tot_lev20'\n",
    "#filename = '20190201_20190228_Grizzly_Bay.tot_lev20' # Feburary, 2019\n",
    "#filename = '20190301_20190331_Grizzly_Bay.tot_lev20' # March, 2019\n",
    "filename = '20190801_20190831_Grizzly_Bay.tot_lev20' # August, 2019\n",
    "\n",
    "# Load Files \n",
    "df = pd.read_csv(filename,skiprows = 6)\n",
    "\n",
    "# Visible light is within 380nm and 700nm\n",
    "listOfHeaders = []\n",
    "for i in df.columns: # Captures only wavelengths between 380nm and 750nm\n",
    "    if ('total' in i.lower()) and ('nm' in i.lower()) and ('aod' in i.lower()):\n",
    "        if 380 <= int(i[4:7]) <= 750:\n",
    "            listOfHeaders.append(i)\n",
    "            #print(i[4:7])\n",
    "print('Number of wavelengths between 380nm and 750nm: ', len(listOfHeaders), '\\n', listOfHeaders)\n",
    "#selectedData = listOfHeaders[11] # AOD_443nm-Total\n",
    "selectedData = listOfHeaders[11] # Increment/Decrement this (0 to 17) to choose a wavelength\n",
    "#selectedData = 'AOD_443nm-Total' # Alternatively, uncomment this and type in the wavelength we want to select.\n",
    "print('Currently Selected Header: ', selectedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de5024b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine dates and times \n",
    "TimeDate = df['Date(dd:mm:yyyy)'] + ' '+df['Time(hh:mm:ss)'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d776513e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dates \n",
    "# Format for the datetime function: datetime(year,month,day)\n",
    "#registeredStartDate = datetime(2019,1,1) # Starts Jan 1st, 2019\n",
    "#registeredStartDate = datetime(2019,2,1) # Starts Feb 1st, 2019\n",
    "#registeredStartDate = datetime(2019,3,1) # Starts Mar 1st, 2019\n",
    "#registeredStartDate = datetime(2019,4,1) # Starts Apr 1st, 2019\n",
    "registeredStartDate = datetime(2019,8,1) # Starts Aug 1st, 2019\n",
    "\n",
    "print(registeredStartDate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6333408",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Parse the data\n",
    "\n",
    "# Store GMT Data \n",
    "GMT = [] # This our GMT_time \n",
    "for i in TimeDate: \n",
    "    dt = datetime.strptime(i,\"%d:%m:%Y %H:%M:%S\") # Stripped information from each row \n",
    "    GMT.append(dt)   \n",
    "    \n",
    "# Convert GMT to Local Time\n",
    "\n",
    "#PDT_time = []  \n",
    "T = pd.Series(data=GMT) # Inputting GMT_time into a series \n",
    "\n",
    "# Converts GMT Dates and Times to Pacific Daylight Savings Time \n",
    "local_Date_time = T.dt.tz_localize('GMT').dt.tz_convert('America/Los_Angeles').dt.tz_localize(None) # This line converts time_zones\n",
    "\n",
    "# Storing local dates and times into list  \n",
    "PDT_Date_time = []\n",
    "for i in local_Date_time: \n",
    "    dt_objects = i.to_pydatetime() # Turns the timestamps to datetime objects \n",
    "    PDT_Date_time.append(dt_objects)\n",
    "    \n",
    "# Find the total seconds starting the week of april 4th \n",
    "\n",
    "PDT_time = [] # official PDT time \n",
    "\n",
    "for i in PDT_Date_time: # Takes time in \n",
    "    \n",
    "    timediff = i.timestamp() - registeredStartDate.timestamp()\n",
    "    \n",
    "    PDT_time.append(timediff)\n",
    "    \n",
    "print(PDT_Date_time[0])\n",
    "print(PDT_Date_time[0]- registeredStartDate)\n",
    "print(PDT_time[0])\n",
    "print(abs(df[selectedData].mean())**2)\n",
    "print(abs(np.mean(df[selectedData]))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40201977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Block implemented by Anthony Castillo \n",
    "\n",
    "# This rootMeanSquareDeviation Function will be used for calculating the error bars of each 3 hour grouping\n",
    "def rootMeanSquareDeviation(integerArray): # Computes the standard deviation of any given integer array\n",
    "    sum = 0\n",
    "    if len(integerArray) == 1:\n",
    "        if integerArray[0] == -999.0:\n",
    "            return 0\n",
    "        else:\n",
    "            return integerArray[0]**5 # This brings the error bars into suit with the rest of the bars.\n",
    "    for x in integerArray:\n",
    "        if x == -999.0: # This value would mess up the calculations, however this x should never be -999.0\n",
    "            x = np.mean(integerArray)\n",
    "        sum = sum + (x-np.mean(integerArray))**2\n",
    "    return (np.sqrt(sum/len(integerArray)) * np.sqrt(2))\n",
    "\n",
    "stdDeviation = rootMeanSquareDeviation(df[selectedData])\n",
    "meanData = abs(df[selectedData].mean())\n",
    "medianData = abs(df[selectedData].median()) # Not used\n",
    "print('RMSD: ', rootMeanSquareDeviation(df[selectedData]))\n",
    "print('Mean of Data: ', meanData)\n",
    "print('Median of Data: ', medianData)\n",
    "\n",
    "# 7pm = 19hrs*3600s = 68,400s \n",
    "# 7am = 7hrs*3600s = 25,200s\n",
    "# 24hrs = 86400s\n",
    "secondsPerDay, upperLimit, lowerLimit = 86400, 19*3600, 7*3600\n",
    "# Here we are grouping elements together that are withing a 3 hour strech of one another\n",
    "df_Mean, df_MeanErrors, threeHourGap, tempTimeAxis, timeAxis = [], [], [], [], []\n",
    "for index, value in enumerate(df[selectedData]): # Iterate thru the data getting the index and value of each element\n",
    "    if lowerLimit > PDT_time[index]-secondsPerDay*(PDT_time[index] // secondsPerDay) or PDT_time[index]-secondsPerDay*(PDT_time[index] // secondsPerDay) > upperLimit: # Skipping indexes which are beyond 7pm.\n",
    "        print('Skipping ', index, ' outside of desired time range.')\n",
    "        continue\n",
    "    if not (meanData-stdDeviation*2 <= abs(value) <= meanData+stdDeviation*2): # Removing data that is too far away from the total mean of the data\n",
    "        print('Removing: ', PDT_time[index], local_Date_time.loc[index], df[selectedData].loc[index], 'Reason: Too far from mean.')\n",
    "        df.drop(index=index, inplace=True)\n",
    "        PDT_time[index] = None\n",
    "        local_Date_time.drop(index=index, inplace=True)\n",
    "        continue\n",
    "    startTime, endTime = 0-3600*2, 10800-3600*2 # 10800 seconds every 3 hours - 3600 to start the counting at 11pm instead of midnight\n",
    "    notAppended, count = True, 0\n",
    "    while notAppended: # Dynamically placing the data into the correct array by resetting startTime, endTime, and count on each new index, value iteration\n",
    "        if startTime <= PDT_time[index] < endTime:\n",
    "            while len(threeHourGap) <= count: # If the array is smaller than what we've counted, we'll add empty arrays to our list\n",
    "                threeHourGap.append([])\n",
    "                tempTimeAxis.append([])\n",
    "            threeHourGap[count].append(value) # Appending the floating point value to the array at location count in threeHourGap\n",
    "            tempTimeAxis[count].append(PDT_time[index]) # Appending the PDT_time to tempTimeAxis at location count \n",
    "            notAppended = False # Flip to end the while loop\n",
    "        elif startTime >= registeredStartDate.timestamp()**3: # If we've gone way over\n",
    "            threeHourGap.append([]) # Adding empty lists.\n",
    "            tempTimeAxis.append([])\n",
    "            notAppended = False\n",
    "        else: # While the PDT_time element is not within the range, we'll incrememnt our count, start, and end times by three hours 3*60*60\n",
    "            count += 1\n",
    "            startTime += 10800\n",
    "            endTime += 10800\n",
    "\n",
    "# Now we'll go back thru and convert the arrays into single point means by passing the array into the rootMeanSquareDeviation function above\n",
    "count = 0\n",
    "for i, v in enumerate(threeHourGap): \n",
    "    if v == []: # Skipping empty arrays\n",
    "        continue\n",
    "    df_Mean.append(np.mean(threeHourGap[i])) # Finding the root mean square of each data array\n",
    "    df_MeanErrors.append(abs(rootMeanSquareDeviation(threeHourGap[i])))\n",
    "    timeAxis.append(tempTimeAxis[i][len(tempTimeAxis[i])//2]) # Get the middle time of each mean data point\n",
    "    count += 1\n",
    "    \n",
    "print(df_Mean)\n",
    "print(df_MeanErrors)\n",
    "\n",
    "# Cleaning up the indexes from the removed data points\n",
    "PDT_temp = []\n",
    "for i in PDT_time: # Adding non-removed attributes to a new list\n",
    "    if i == None:\n",
    "        continue\n",
    "    else:\n",
    "        PDT_temp.append(i)\n",
    "PDT_time.clear() # Clear old list\n",
    "PDT_time = PDT_temp # Set old list to new list\n",
    "df.reset_index(drop=True, inplace=True) # Re-does the indexes for the pandas.Series, takes dropped rows into consideration.\n",
    "local_Date_time.reset_index(drop=True, inplace=True) # Re-does the indexes for the pandas.Series, takes dropped rows into consideration.\n",
    "\n",
    "print('Lenght of datasets are: ', len(PDT_time), len(local_Date_time), len(df))\n",
    "\n",
    "del threeHourGap, tempTimeAxis, startTime, endTime, notAppended, count # Clearing variables that are no longer needed. Not needed, but clears up memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8f9275",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Sacramento River/Grizzly Bay: Feburary 1st - Feburary 28th 2019\n",
    "fig = plt.figure(figsize = (12,6))\n",
    "\n",
    "plt.errorbar(timeAxis,df_Mean,yerr=df_MeanErrors,fmt='.',color='black',markersize=5,elinewidth=1,capsize=2,label='AOD')\n",
    "plt.plot(PDT_time,df[selectedData],'.',color='C0',markersize=3,alpha=0.4) # C0, C1, C2, C3 cycles through the default colors\n",
    "plt.plot(timeAxis,df_Mean,'.',color='black')\n",
    "\n",
    "# Change the title to fit your station\n",
    "title0 = 'Sacramento River/Grizzly Bay (' + selectedData + '): Feburary 1st - Feburary 28th 2019'\n",
    "title1 = 'Sacramento River/Grizzly Bay (' + selectedData + '): August 1st - August 31th 2019'\n",
    "title2 = 'Sacramento River/Grizzly Bay (' + selectedData + '): January 1st - January 31th 2019'\n",
    "title3 = 'Sacramento River/Grizzly Bay (' + selectedData + '): March 1st - March 31th 2019'\n",
    "title4 = 'Sacramento River/Grizzly Bay (' + selectedData + '): April 1st - April 30th 2019'\n",
    "\n",
    "days = 31 # Adjust the number of days to fit the current month\n",
    "\n",
    "plt.title(title1,fontsize = 14)\n",
    "\n",
    "plt.xlabel('Day(s)',fontsize = 14)\n",
    "\n",
    "plt.ylabel('Optical Depth',fontsize = 14)\n",
    "\n",
    "x_label = [str(i+1) for i in range(days)]\n",
    "\n",
    "plt.xticks(np.arange(days)*24*3600,x_label,fontsize = 12) # Sets x-ticks for the week  \n",
    "\n",
    "# Set xticks to labels \n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "plt.xlim([24*3600*0,24*3600*days])\n",
    "#plt.ylim([-0.1,0.9]) # Not needed, the plots should automatically adjust based on the available data\n",
    "\n",
    "# To save the graph, adjust the names of the folders below and the name of the files themselves.\n",
    "#filename = 'Sacramento_River_2019_January_' + selectedData\n",
    "#location = 'graphs\\\\Sacramento River 2019 January\\\\' + filename\n",
    "\n",
    "#filename = 'Sacramento_River_2019_Feburary' + selectedData\n",
    "#location = 'graphs\\\\Sacramento River 2019 Feburary\\\\' + filename\n",
    "\n",
    "#filename = 'Sacramento_River_2019_March_' + selectedData\n",
    "#location = 'graphs\\\\Sacramento River 2019 March\\\\' + filename\n",
    "\n",
    "#filename = 'Sacramento_River_2019_April' + selectedData\n",
    "#location = 'graphs\\\\Sacramento River 2019 April\\\\' + filename\n",
    "\n",
    "filename = 'Sacramento_River_2019_August_' + selectedData\n",
    "location = 'graphs\\\\Sacramento River 2019 August\\\\' + filename\n",
    "\n",
    "plt.savefig(location) # Saves the plot to a .png file.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6015f6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in list(df): # Displays all of the aod total wavelengths available in this dataset\n",
    "    if ('total' in i.lower()) and ('aod' in i.lower()) and ('nm' in i.lower()):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134b7a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional Debugging, displaying data and times\n",
    "print(len(PDT_time), len(local_Date_time), len(df))\n",
    "print(local_Date_time.loc[len(local_Date_time)/2])\n",
    "print(df[selectedData].loc[len(df[selectedData])/2])\n",
    "\n",
    "for i, v in enumerate(PDT_time):\n",
    "    print(v, v//10800, local_Date_time.loc[i], df[selectedData].loc[i])#datetime.fromtimestamp(v), df[selectedData][i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "e4c414fde0947f8aa31626b0dd4d6cba250e1b428c01214d2a5a38724c9c5929"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
